{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c1f49861",
   "metadata": {},
   "source": [
    "1.\n",
    "data gathering,data cleaning,EDA,data Preprocessing,data visualization,model building\n",
    "Data preprocessing is required tasks for cleaning the data and making it suitable for a machine learning model which also \n",
    "increases the accuracy and efficiency of a machine learning model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5792d38a",
   "metadata": {},
   "source": [
    "2.\n",
    "Quantitative data are measures of values or counts and are expressed as numbers.\n",
    "\n",
    "Quantitative data are data about numeric variables (e.g. how many; how much; or how often).\n",
    "\n",
    "Qualitative data are measures of 'types' and may be represented by a name, symbol, or a number code.\n",
    "\n",
    "Qualitative data are data about categorical variables (e.g. what type)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63eb0106",
   "metadata": {},
   "source": [
    "4.\n",
    "Understanding Which Processes Need Automation\n",
    "Inadequate Infrastructure\n",
    "Lack of Quality Data\n",
    "Lack of Skilled Resources\n",
    "These steps essentially are just a combination of typical steps to solve a problem as well as the Software Development Life Cycle.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Understand the Problem\n",
    "This is pretty straight forward. You need to be able to define and understand what problem you’re trying to solve. You should have a higher perspective of what real-world problem you want to help solve. If done well, this part gets you really motivated about everything else that is to follow.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Review Dataset\n",
    "This step is ignored so often and is so underrated that I cannot emphasize the importance of it enough. You really need to carefully look at your dataset in order to analyze not only if you have enough data or not, if you need more data or if/how you can augment it but also see the quality of the data.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "The quality of the data fundamentally determines if you will be able to solve the problem at all or not. For instance, you’re working at a project where the problem is to be able to classify certain rarely found lab equipment images. You thought, okay, I have so many of these images, I’ll turn up a GPU and feed these images to a neural network and see what happens. When you test the neural network, you find out that it doesn’t have the slightest idea of what is what. What happened? Aren’t neural networks supposed to magically work? No, they are not! Your data had blurry and dark images. Some pictures didn’t even have the whole equipment as part of the image. You didn’t review your dataset and spent all your resources without realizing the dataset’s capacity to potentially train a model.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Set an end goal\n",
    "Rephrase: Set a realistic engineering end goal. At the business side, we make big promises of what revolutionary product we want to make. But only after reviewing the dataset, can you have a real perspective of what is really possible. Therefore, it is very significant to get a general idea of things and then setting an end goal.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "List alternate solutions\n",
    "This step can be approached differently in different situations. If you have seen a similar problem in the past, you will have a more accurate idea of what kind of solutions can help in the situation.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "If you’re working on a relatively new problem, this step really puts your R&D gears to use. You will need to research what kind of solutions can be suitable depending on the problem, the dataset and the kind of time you have to be able to do your predictions/classifications etc. (For instance, YOLO works fast but is relatively less accurate for similar looking objects while RetinaNet is slower but more accurate)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Select a solution\n",
    "You can choose a subset of your dataset and train your dataset for a lesser number of iterations to see how well your model is learning.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Based on this, you are in a very good position to select which solution is better and use that for implementation.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Implementation\n",
    "This is pretty straight-forward. Implement your solution and implement it well. Try following your language standards as much as you can. Make sure your code is readable. Use OOP if you can. Reuse things if you can and don’t reinvent the wheel.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Evaluation\n",
    "Always select an evaluation metric relevant to your goal. Benchmark your solution on the basis of your selected evaluation metric. Just testing it on a bunch of test data is not enough."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df7a451b",
   "metadata": {},
   "source": [
    "5.\n",
    "Encoding Categorical Data\n",
    "You will now learn different techniques to encode the categorical features to numeric quantities. To keep it simple, you will apply these encoding methods only on the carrier column. However, the same approach can be extended to all columns.\n",
    "\n",
    "The techniques that you'll cover are the following:\n",
    "\n",
    "Replacing values\n",
    "Encoding labels\n",
    "One-Hot encoding\n",
    "Binary encoding\n",
    "Backward difference encoding\n",
    "Miscellaneous features\n",
    "Replace Values\n",
    "Let's start with the most basic method, which is just replacing the categories with the desired numbers. This can be achieved with the help of the replace() function in pandas. The idea is that you have the liberty to choose whatever numbers you want to assign to the categories according to the business use case.\n",
    "\n",
    "You will now create a dictionary which contains mapping numbers for each category in the carrier column:\n",
    "\n",
    "replace_map = {'carrier': {'AA': 1, 'AS': 2, 'B6': 3, 'DL': 4,\n",
    "                                  'F9': 5, 'HA': 6, 'OO': 7 , 'UA': 8 , 'US': 9,'VX': 10,'WN': 11}}\n",
    "Note that defining a mapping via a hard coded dictionary is easy when the number of categories is low, like in this case which is 11. You can achieve the same mapping with the help of dictionary comprehensions as shown below. This will be useful when the categories count is high and you don't want to type out each mapping. You will store the category names in a list called labels and then zip it to a seqeunce of numbers and iterate over it.\n",
    "\n",
    "labels = cat_df_flights['carrier'].astype('category').cat.categories.tolist()\n",
    "replace_map_comp = {'carrier' : {k: v for k,v in zip(labels,list(range(1,len(labels)+1)))}}\n",
    "\n",
    "print(replace_map_comp)\n",
    "{'carrier': {'AA': 1, 'OO': 7, 'DL': 4, 'F9': 5, 'B6': 3, 'US': 9, 'AS': 2, 'WN': 11, 'VX': 10, 'HA': 6, 'UA': 8}}\n",
    "Throughout this tutorial, you will be making a copy of the dataset via the .copy() method to practice each encoding technique to ensure that the original DataFrame stays intact and whatever changes you are doing happen only in the copied one.\n",
    "\n",
    "cat_df_flights_replace = cat_df_flights.copy()\n",
    "Use the replace() function on the DataFrame by passing the mapping dictionary as argument:\n",
    "\n",
    "cat_df_flights_replace.replace(replace_map_comp, inplace=True)\n",
    "\n",
    "print(cat_df_flights_replace.head())\n",
    "\n",
    "\n",
    "As you can observe, you have encoded the categories with the mapped numbers in your DataFrame.\n",
    "\n",
    "You can also check the dtype of the newly encoded column, which is now converted to integers.\n",
    "\n",
    "print(cat_df_flights_replace['carrier'].dtypes)\n",
    "int64\n",
    "Tip: in Python, it's a good practice to typecast categorical features to a category dtype because they make the operations on such columns much faster than the object dtype. You can do the typecasting by using .astype() method on your columns like shown below:\n",
    "\n",
    "cat_df_flights_lc = cat_df_flights.copy()\n",
    "cat_df_flights_lc['carrier'] = cat_df_flights_lc['carrier'].astype('category')\n",
    "cat_df_flights_lc['origin'] = cat_df_flights_lc['origin'].astype('category')                                                              \n",
    "\n",
    "print(cat_df_flights_lc.dtypes)\n",
    "carrier    category\n",
    "tailnum      object\n",
    "origin     category\n",
    "dest         object\n",
    "dtype: object\n",
    "You can validate the faster operation of the category dtype by timing the execution time of the same operation done on a DataFrame with columns as category dtype and object dtype by using the time library.\n",
    "\n",
    "Let's say you want to calculate the number of flights for each carrier from each origin places, you can use the .groupby() and .count() methods on your DataFrame to do so.\n",
    "\n",
    "import time\n",
    "%timeit cat_df_flights.groupby(['origin','carrier']).count() #DataFrame with object dtype columns\n",
    "10 loops, best of 3: 28.6 ms per loop\n",
    "%timeit cat_df_flights_lc.groupby(['origin','carrier']).count() #DataFrame with category dtype columns\n",
    "10 loops, best of 3: 20.1 ms per loop\n",
    "Note that the DataFrame with category dtype is much faster.\n",
    "\n",
    "Label Encoding\n",
    "Another approach is to encode categorical values with a technique called \"label encoding\", which allows you to convert each value in a column to a number. Numerical labels are always between 0 and n_categories-1.\n",
    "\n",
    "You can do label encoding via attributes .cat.codes on your DataFrame's column.\n",
    "\n",
    "cat_df_flights_lc['carrier'] = cat_df_flights_lc['carrier'].cat.codes\n",
    "cat_df_flights_lc.head() #alphabetically labeled from 0 to 10\n",
    "\n",
    "\n",
    "Sometimes, you might just want to encode a bunch of categories within a feature to some numeric value and encode all the other categories to some other numeric value.\n",
    "\n",
    "You could do this by using numpy's where() function like shown below. You will encode all the US carrier flights to value 1 and other carriers to value 0. This will create a new column in your DataFrame with the encodings. Later, if you want to drop the original column, you can do so by using the drop() function in pandas.\n",
    "\n",
    "cat_df_flights_specific = cat_df_flights.copy()\n",
    "cat_df_flights_specific['US_code'] = np.where(cat_df_flights_specific['carrier'].str.contains('US'), 1, 0)\n",
    "\n",
    "cat_df_flights_specific.head()\n",
    "\n",
    "\n",
    "You can achieve the same label encoding using scikit-learn's LabelEncoder:\n",
    "\n",
    "cat_df_flights_sklearn = cat_df_flights.copy()\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "lb_make = LabelEncoder()\n",
    "cat_df_flights_sklearn['carrier_code'] = lb_make.fit_transform(cat_df_flights['carrier'])\n",
    "\n",
    "cat_df_flights_sklearn.head() #Results in appending a new column to df\n",
    "\n",
    "\n",
    "Label encoding is pretty much intuitive and straight-forward and may give you a good performance from your learning algorithm, but it has as disadvantage that the numerical values can be misinterpreted by the algorithm. Should the carrier US (encoded to 8) be given 8x more weight than the carrier AS (encoded to 1) ?\n",
    "\n",
    "To solve this issue there is another popular way to encode the categories via something called one-hot encoding.\n",
    "\n",
    "One-Hot encoding\n",
    "The basic strategy is to convert each category value into a new column and assign a 1 or 0 (True/False) value to the column. This has the benefit of not weighting a value improperly.\n",
    "\n",
    "There are many libraries out there that support one-hot encoding but the simplest one is using pandas' .get_dummies() method.\n",
    "\n",
    "This function is named this way because it creates dummy/indicator variables (1 or 0). There are mainly three arguments important here, the first one is the DataFrame you want to encode on, second being the columns argument which lets you specify the columns you want to do encoding on, and third, the prefix argument which lets you specify the prefix for the new columns that will be created after encoding.\n",
    "\n",
    "cat_df_flights_onehot = cat_df_flights.copy()\n",
    "cat_df_flights_onehot = pd.get_dummies(cat_df_flights_onehot, columns=['carrier'], prefix = ['carrier'])\n",
    "\n",
    "print(cat_df_flights_onehot.head())\n",
    "\n",
    "\n",
    "As you can see, the column carrier_AS gets value 1 at the 0th and 4th observation points as those points had the AS category labeled in the original DataFrame. Likewise for other columns also.\n",
    "\n",
    "scikit-learn also supports one hot encoding via LabelBinarizer and OneHotEncoder in its preprocessing module (check out the details here). Just for the sake of practicing you will do the same encoding via LabelBinarizer:\n",
    "\n",
    "cat_df_flights_onehot_sklearn = cat_df_flights.copy()\n",
    "\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "\n",
    "lb = LabelBinarizer()\n",
    "lb_results = lb.fit_transform(cat_df_flights_onehot_sklearn['carrier'])\n",
    "lb_results_df = pd.DataFrame(lb_results, columns=lb.classes_)\n",
    "\n",
    "print(lb_results_df.head())\n",
    "\n",
    "\n",
    "Note that this lb_results_df resulted in a new DataFrame with only the one hot encodings for the feature carrier. This needs to be concatenated back with the original DataFrame, which can be done via pandas' .concat() method. The axis argument is set to 1 as you want to merge on columns.\n",
    "\n",
    "result_df = pd.concat([cat_df_flights_onehot_sklearn, lb_results_df], axis=1)\n",
    "\n",
    "print(result_df.head())\n",
    "\n",
    "\n",
    "While one-hot encoding solves the problem of unequal weights given to categories within a feature, it is not very useful when there are many categories, as that will result in formation of as many new columns, which can result in the curse of dimensionality. The concept of the “curse of dimensionality” discusses that in high-dimensional spaces some things just stop working properly.\n",
    "\n",
    "Binary Encoding\n",
    "This technique is not as intuitive as the previous ones. In this technique, first the categories are encoded as ordinal, then those integers are converted into binary code, then the digits from that binary string are split into separate columns. This encodes the data in fewer dimensions than one-hot.\n",
    "\n",
    "You can do binary encoding via a number of ways but the simplest one is using the category_encoders library. You can install category_encoders via pip install category_encoders on cmd or just download and extract the .tar.gz file from the site.\n",
    "\n",
    "You have to first import the category_encoders library after installing it. Invoke the BinaryEncoder function by specifying the columns you want to encode and then call the .fit_transform() method on it with the DataFrame as the argument.\n",
    "\n",
    "cat_df_flights_ce = cat_df_flights.copy()\n",
    "\n",
    "import category_encoders as ce\n",
    "\n",
    "encoder = ce.BinaryEncoder(cols=['carrier'])\n",
    "df_binary = encoder.fit_transform(cat_df_flights_ce)\n",
    "\n",
    "df_binary.head()\n",
    "\n",
    "\n",
    "Notice that four new columns are created in place of the carrier column with binary encoding for each category in the feature.\n",
    "\n",
    "Note that category_encoders is a very useful library for encoding categorical columns. Not only does it support one-hot, binary and label encoding, but also other advanced encoding methods like Helmert contrast, polynomial contrast, backward difference, etc.\n",
    "\n",
    "5. Backward Difference Encoding\n",
    "This technique falls under the contrast coding system for categorical features. A feature of K categories, or levels, usually enters a regression as a sequence of K-1 dummy variables. In backward difference coding, the mean of the dependent variable for a level is compared with the mean of the dependent variable for the prior level. This type of coding may be useful for a nominal or an ordinal variable.\n",
    "\n",
    "If you want to learn other contrast coding methods you can check out this resource.\n",
    "\n",
    "The code structure is pretty much the same as any method in the category_encoders library, just this time you will call BackwardDifferenceEncoder from it:\n",
    "\n",
    "encoder = ce.BackwardDifferenceEncoder(cols=['carrier'])\n",
    "df_bd = encoder.fit_transform(cat_df_flights_ce)\n",
    "\n",
    "df_bd.head()\n",
    "\n",
    "\n",
    "The interesting thing here is that you can see that the results are not the standard 1’s and 0’s you saw in the dummy encoding examples but rather regressed continuous values.\n",
    "\n",
    "Miscellaneous Features\n",
    "Sometimes you may encounter categorical feature columns which specify the ranges of values for observation points, for example, the age column might be described in the form of categories like 0-20, 20-40 and so on.\n",
    "\n",
    "While there can be a lot of ways to deal with such features, the most common ones are either split these ranges into two separate columns or replace them with some measure like the mean of that range.\n",
    "\n",
    "You will first create a dummy DataFrame which has just one feature age with ranges specified using the pandas DataFrame function. Then you will split the column on the delimeter - into two columns start and end using split() with a lambda() function. If you want to learn more about lambda functions, check out this tutorial.\n",
    "\n",
    "dummy_df_age = pd.DataFrame({'age': ['0-20', '20-40', '40-60','60-80']})\n",
    "dummy_df_age['start'], dummy_df_age['end'] = zip(*dummy_df_age['age'].map(lambda x: x.split('-')))\n",
    "\n",
    "dummy_df_age.head()\n",
    "\n",
    "\n",
    "To replace the range with its mean, you will write a split_mean() function which basically takes one range at a time, splits it, then calculates the mean and returns it. To apply a certain function to all the entities of a column you will use the .apply() method:\n",
    "\n",
    "dummy_df_age = pd.DataFrame({'age': ['0-20', '20-40', '40-60','60-80']})\n",
    "\n",
    "def split_mean(x):\n",
    "    split_list = x.split('-')\n",
    "    mean = (float(split_list[0])+float(split_list[1]))/2\n",
    "    return mean\n",
    "\n",
    "dummy_df_age['age_mean'] = dummy_df_age['age'].apply(lambda x: split_mean(x))\n",
    "\n",
    "dummy_df_age.head()\n",
    "\n",
    "\n",
    "\n",
    "Dealing with Categorical Features in Big Data with Spark\n",
    "Now you will learn how to read a dataset in Spark and encode categorical variables in Apache Spark's Python API, Pyspark. But before that it's good to brush up on some basic knowledge about Spark.\n",
    "\n",
    "Spark is a platform for cluster computing. It lets you spread data and computations over clusters with multiple nodes. Splitting up your data makes it easier to work with very large datasets because each node only works with a small amount of data.\n",
    "\n",
    "As each node works on its own subset of the total data, it also carries out a part of the total calculations required, so that both data processing and computations are performed in parallel over the nodes in the cluster.\n",
    "\n",
    "Deciding whether or not Spark is the best solution for your problem takes some experience, but you can consider questions like:\n",
    "\n",
    "Is my data too big to work with on a single machine?\n",
    "Can my calculations be easily parallelized?\n",
    "The first step in using Spark is connecting to a cluster. In practice, the cluster will be hosted on a remote machine that's connected to all other nodes. There will be one computer, called the master that manages splitting up the data and the computations. The master is connected to the rest of the computers in the cluster, which are called slaves. The master sends the slaves data and calculations to run, and they send their results back to the master.\n",
    "\n",
    "When you're just getting started with Spark, it's simpler to just run a cluster locally. If you wish to run Spark on a cluster and use Jupyter Notebook, you can check out this blog.\n",
    "\n",
    "If you wish to learn more about Spark, check out this great tutorial which covers almost everything about it, or DataCamp's Introduction to PySpark course.\n",
    "\n",
    "The first step in Spark programming is to create a SparkContext. SparkContext is required when you want to execute operations in a cluster. SparkContext tells Spark how and where to access a cluster. You'll start by importing SparkContext.\n",
    "\n",
    "from pyspark import SparkContext\n",
    "sc = SparkContext()\n",
    "Note that if you are working on Spark's interactive shell then you don't have to import SparkContext as it will already be in your environment as sc.\n",
    "\n",
    "To start working with Spark DataFrames, you first have to create a SparkSession object from your SparkContext. You can think of the SparkContext as your connection to the cluster and the SparkSession as your interface with that connection.\n",
    "\n",
    "Note that if you are working in Spark's interactive shell you'll have a SparkSession called spark available in your workspace!\n",
    "\n",
    "from pyspark.sql import SparkSession as spark\n",
    "Once you've created a SparkSession, you can start poking around to see what data is in your cluster.\n",
    "\n",
    "Your SparkSession has an attribute called catalog which lists all the data inside the cluster. This attribute has a few methods for extracting different pieces of information.\n",
    "\n",
    "One of the most useful is the .listTables() method, which returns the names of all the tables in your cluster as a list.\n",
    "\n",
    "print(spark.catalog.listTables())\n",
    "[]\n",
    "Your catalog is currently empty!\n",
    "\n",
    "You will now load the flights dataset in the Spark DataFrame.\n",
    "\n",
    "To read a .csv file and create a Spark DataFrame you can use the .read attribute of your SparkSession object. Here, apart from reading the csv file, you have to additionally specify the headers option to be True, since you have column names in the dataset. Also, the inferSchema argument is set to True, which basically peeks at the first row of the data to determine the fields' names and types.\n",
    "\n",
    "spark_flights = spark.read.format(\"csv\").option('header',True).load('Downloads/datasets/nyc_flights/flights.csv',inferSchema=True)\n",
    "To check the contents of your DataFrame you can run the .show() method on the DataFrame.\n",
    "\n",
    "spark_flights.show(3)\n",
    "+----+-----+---+--------+---------+--------+---------+-------+-------+------+------+----+--------+--------+----+------+\n",
    "|year|month|day|dep_time|dep_delay|arr_time|arr_delay|carrier|tailnum|flight|origin|dest|air_time|distance|hour|minute|\n",
    "+----+-----+---+--------+---------+--------+---------+-------+-------+------+------+----+--------+--------+----+------+\n",
    "|2014|    1|  1|       1|       96|     235|       70|     AS| N508AS|   145|   PDX| ANC|     194|    1542|   0|     1|\n",
    "|2014|    1|  1|       4|       -6|     738|      -23|     US| N195UW|  1830|   SEA| CLT|     252|    2279|   0|     4|\n",
    "|2014|    1|  1|       8|       13|     548|       -4|     UA| N37422|  1609|   PDX| IAH|     201|    1825|   0|     8|\n",
    "+----+-----+---+--------+---------+--------+---------+-------+-------+------+------+----+--------+--------+----+------+\n",
    "only showing top 3 rows\n",
    "If you wish to convert a pandas DataFrame to a Spark DataFrame, use the .createDataFrame() method on your SparkSession object with the DataFrame's name as argument.\n",
    "\n",
    "To have a look at the schema of the DataFrame you can invoke .printSchema() as follows:\n",
    "\n",
    "spark_flights.printSchema()\n",
    "root\n",
    " |-- year: integer (nullable = true)\n",
    " |-- month: integer (nullable = true)\n",
    " |-- day: integer (nullable = true)\n",
    " |-- dep_time: string (nullable = true)\n",
    " |-- dep_delay: string (nullable = true)\n",
    " |-- arr_time: string (nullable = true)\n",
    " |-- arr_delay: string (nullable = true)\n",
    " |-- carrier: string (nullable = true)\n",
    " |-- tailnum: string (nullable = true)\n",
    " |-- flight: integer (nullable = true)\n",
    " |-- origin: string (nullable = true)\n",
    " |-- dest: string (nullable = true)\n",
    " |-- air_time: string (nullable = true)\n",
    " |-- distance: integer (nullable = true)\n",
    " |-- hour: string (nullable = true)\n",
    " |-- minute: string (nullable = true)\n",
    "Note that Spark doesn't always guess the data type of the columns right and you can see that some of the columns (arr_delay, air_time, etc.) which seem to have numeric values are read as strings rather than integers or floats, due to the presence of missing values.\n",
    "\n",
    "At this point, if you check the data in your cluster using the .catalog attribute and the .listTables() method like you did before, you will find it's still empty. This is because you DataFrame is currently stored locally, not in the SparkSession catalog.\n",
    "\n",
    "To access the data in this way, you have to save it as a temporary table. You can do so by using the .createOrReplaceTempView() method. This method registers the DataFrame as a table in the catalog, but as this table is temporary, it can only be accessed from the specific SparkSession used to create the Spark DataFrame.\n",
    "\n",
    "spark_flights.createOrReplaceTempView(\"flights_temp\")\n",
    "Print the tables in catalog again:\n",
    "\n",
    "print(spark.catalog.listTables())\n",
    "[Table(name=u'flights_temp', database=None, description=None, tableType=u'TEMPORARY', isTemporary=True)]\n",
    "Now you have registered the flight_temp table as a temporary table in your catalog.\n",
    "\n",
    "Now that you have gotten your hands dirty with a little bit of PySpark code, it's time to see how to encode categorical features. To keep things neat, you will create a new DataFrame which consists of only the carrier column by using the .select() method.\n",
    "\n",
    "carrier_df = spark_flights.select(\"carrier\")\n",
    "carrier_df.show(5)\n",
    "+-------+\n",
    "|carrier|\n",
    "+-------+\n",
    "|     AS|\n",
    "|     US|\n",
    "|     UA|\n",
    "|     US|\n",
    "|     AS|\n",
    "+-------+\n",
    "only showing top 5 rows\n",
    "The two most common ways to encode categorical features in Spark are using StringIndexer and OneHotEncoder.\n",
    "\n",
    "StringIndexer encodes a string column of labels to a column of label indices. The indices are in [0, numLabels] ordered by label frequencies, so the most frequent label gets index 0. This is similar to label encoding in pandas.\n",
    "You will start by importing the StringIndexer class from the pyspark.ml.feature module. The main arguments inside StringIndexer are inputCol and outputCol, which are self-explanatory. After you create the StringIndex object you call the .fit() and .transform() methods with the DataFrame as the argument passed as shown:\n",
    "\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "carr_indexer = StringIndexer(inputCol=\"carrier\",outputCol=\"carrier_index\")\n",
    "carr_indexed = carr_indexer.fit(carrier_df).transform(carrier_df)\n",
    "\n",
    "carr_indexed.show(7)\n",
    "+-------+-------------+\n",
    "|carrier|carrier_index|\n",
    "+-------+-------------+\n",
    "|     AS|          0.0|\n",
    "|     US|          6.0|\n",
    "|     UA|          4.0|\n",
    "|     US|          6.0|\n",
    "|     AS|          0.0|\n",
    "|     DL|          3.0|\n",
    "|     UA|          4.0|\n",
    "+-------+-------------+\n",
    "only showing top 7 rows\n",
    "Since AS was the most frequent category in the carrier column, it got the index 0.0.\n",
    "\n",
    "OneHotEncoder: as you already read before, one-hot encoding maps a categorical feature, represented as a label index, to a binary vector with at most a single one-value indicating the presence of a specific feature value from among the set of all feature values.\n",
    "For example, with 5 categories, an input value of 2.0 would map to an output vector of [0.0, 0.0, 1.0, 0.0]. The last category is not included by default (configurable via OneHotEncoder .dropLast because it makes the vector entries sum up to one, and hence linearly dependent. That means that an input value of 4.0 would map to [0.0, 0.0, 0.0, 0.0].\n",
    "\n",
    "Note that this is different from scikit-learn's OneHotEncoder, which keeps all categories. The output vectors are sparse.\n",
    "\n",
    "For a string type like in this case, it is common to encode features using StringIndexer first, here carrier_index. Then pass that column to the OneHotEncoder class.\n",
    "\n",
    "The code is shown below.\n",
    "\n",
    "carrier_df_onehot = spark_flights.select(\"carrier\")\n",
    "\n",
    "from pyspark.ml.feature import OneHotEncoder, StringIndexer\n",
    "\n",
    "stringIndexer = StringIndexer(inputCol=\"carrier\", outputCol=\"carrier_index\")\n",
    "model = stringIndexer.fit(carrier_df_onehot)\n",
    "indexed = model.transform(carrier_df_onehot)\n",
    "encoder = OneHotEncoder(dropLast=False, inputCol=\"carrier_index\", outputCol=\"carrier_vec\")\n",
    "encoded = encoder.transform(indexed)\n",
    "\n",
    "encoded.show(7)\n",
    "+-------+-------------+--------------+\n",
    "|carrier|carrier_index|   carrier_vec|\n",
    "+-------+-------------+--------------+\n",
    "|     AS|          0.0|(11,[0],[1.0])|\n",
    "|     US|          6.0|(11,[6],[1.0])|\n",
    "|     UA|          4.0|(11,[4],[1.0])|\n",
    "|     US|          6.0|(11,[6],[1.0])|\n",
    "|     AS|          0.0|(11,[0],[1.0])|\n",
    "|     DL|          3.0|(11,[3],[1.0])|\n",
    "|     UA|          4.0|(11,[4],[1.0])|\n",
    "+-------+-------------+--------------+\n",
    "only showing top 7 rows\n",
    "Note that OneHotEncoder has created a vector for each category which can then be processed further by your machine learning pipeline.\n",
    "\n",
    "There are some more methods available in Spark like VectorIndexer, but you have already mastered the most popular ones. If you wish to explore more, check out Spark's fantastic documentation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9f0c639",
   "metadata": {},
   "source": [
    "6.\n",
    "if some variables having missing values it will lead to lack of information.in such cases we can use mean,median,mode imputaion."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "378f6a8c",
   "metadata": {},
   "source": [
    "7.                          ***Handling Missing values of Numerical Variables***\n",
    "\n",
    " 1. Handling the Missing Values.(Must be Good at pandas,matplotlib,Numpy,seaborn)\n",
    "\n",
    "    1.Missing Completely at random:(when data is MCAR,there is no relation between data missing\n",
    "      and any other values)\n",
    "        1.Mean,median and mode imputation\n",
    "             here we just fill nan values with mean,median or mode value.\n",
    "        2.Random Sample imputation:\n",
    "             def impute_nan(df,variable,median):\n",
    "                 df[variable+\"_median\"]=df[variable].fillna(median)\n",
    "                 df[variable+\"_random\"]=df[variable]\n",
    "          ##It will have the random sample to fill the na\n",
    "                 random_sample=df[variable].dropna().sample(df[variable].isnull().sum(),random_state=0)\n",
    "          #here we do dropna coz we do not want samples of nan values   \n",
    "          #here we include sum of null values to create how many samples to fill\n",
    "          ##pandas need to have same index in order to merge the dataset\n",
    "                 random_sample.index=df[df[variable].isnull()].index\n",
    "                 df.loc[df[variable].isnull(),variable+'_random']=random_sample\n",
    "          #here we are saying where feature is null,in that index and in new feature variable_random,\n",
    "          fill all random_sample values.\n",
    "          \n",
    "    \t\n",
    "    2.Missing  Data Not  at random:(When data is MNAR,there is some relation between missing\n",
    "      values and other values ,observed or missing within the dataset.)\n",
    "        1.Capturing nan values with new feature(Here the new feature is created \n",
    "          where null values represented by 1 and other's 0 and in real feature the \n",
    "          null values replaced by mean,median or mode)\n",
    "        2.End of distribution imputation(here the values which is after 3rd std deviation\n",
    "          is calculated (df[variable].mean()+3*df[variable].std()) and\n",
    "          use to fill nan values.\n",
    "   \n",
    "    3.Missing at random:(when data is MAR,the probability of being missing is same in\n",
    "      whole dataset. \n",
    "\n",
    "\n",
    "\n",
    "                                    ***Handling Categorical Missing Values***\n",
    "\n",
    "    1.Adding new feature to capture NAN\n",
    "       in this method we creat new feature where we replace null by 1 and other's 0 and in real feature the \n",
    "       null values replaced by mode.\n",
    "\n",
    "    2.frequent Category imputation\n",
    "       here,we just simply replace the missing values with frequent value or mode().\n",
    "\n",
    "    3.if we have so many unique categories ,then we create new feature and \n",
    "      replace null values with \"Missing\" as a category.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70d8d58e",
   "metadata": {},
   "source": [
    "8.They are Data Cleaning/Cleansing, Data Integration, Data Transformation, and Data Reduction.\n",
    "\n",
    "Dimensionality reduction refers to techniques that reduce the number of input variables in a dataset. More input features often make a predictive modeling task more challenging to model, more generally referred to as the curse of dimensionality.\n",
    "\n",
    "Feature selection is the process of reducing the number of input variables when developing a predictive model. It is desirable to reduce the number of input variables to both reduce the computational cost of modeling and, in some cases, to improve the performance of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7be9dea8",
   "metadata": {},
   "source": [
    "9.\n",
    "The interquartile range is calculated in much the same way as the range. All you do to find it is subtract the first quartile from the third quartile: IQR = Q3 – Q1. The interquartile range shows how the data is spread about the median."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69305b85",
   "metadata": {},
   "source": [
    "10."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
